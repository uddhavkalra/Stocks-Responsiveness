{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326d4cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed5a9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load funds index/non-index data\n",
    "crsp_data = pd.read_excel(\"Data/CRSP_fund.xlsx\")\n",
    "crsp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e80a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(crsp_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2164eac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset data and rename columns for ease\n",
    "crsp_data2 = crsp_data[['Date','Fund Identifier','CUSIP (8-digit)','Fund Name', 'index_fund_flag']]\n",
    "crsp_cols = ['Date', 'fundid(CRSP)', 'fund_CUSIP', 'fundname(CRSP)', 'indexflag']\n",
    "crsp_data2.columns = crsp_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00ffc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only select the index funds (B and D)\n",
    "index_flag_funds = crsp_data2[crsp_data2['indexflag'].isin(['B', 'D'])]\n",
    "index_flag_funds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3359d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load funds holdings data\n",
    "lseg_data, lseg_meta = pyreadstat.read_dta(\"Data/LSEG_fund.dta\")\n",
    "lseg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5881fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lseg_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed38a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset data and rename columns for ease\n",
    "lseg_data2 = lseg_data[lseg_data['shrout2'].isna() == False]\n",
    "lseg_data3 = lseg_data2[['fdate','fundno', 'fundname','cusip','stkname', 'ticker','shrout2']]\n",
    "lseg_cols = ['Date', 'fundid(LSEG)', 'fundname(LSEG)', 'stock_CUSIP', 'stock_name', 'ticker', 'shares_outstanding']\n",
    "lseg_data3.columns = lseg_cols\n",
    "lseg_data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7195cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert date columns to datetime format\n",
    "index_flag_funds['Date'] = pd.to_datetime(index_flag_funds['Date'])\n",
    "lseg_data3['Date'] = pd.to_datetime(lseg_data3['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece7081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter data from 2021 onwards\n",
    "lseg_data3 = lseg_data3[lseg_data3['Date'] >= \"2021-01-01\"]\n",
    "lseg_data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2f0d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load WFICN to CRSP mapping data\n",
    "CRSP_WFICN = pd.read_excel(\"Data/CRSP_WFICN.xlsx\")\n",
    "CRSP_WFICN2 = CRSP_WFICN[['Fund Identifier', 'Wharton Financial Instution Center Number']]\n",
    "CRSP_WFICN2.columns = ['fundid(CRSP)', 'wficn']\n",
    "CRSP_WFICN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec41da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with index funds data\n",
    "index_flag_funds2 = pd.merge(index_flag_funds, CRSP_WFICN2, how = \"inner\", on=\"fundid(CRSP)\")\n",
    "index_flag_funds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbee634",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load WFICN to LSEG mapping data\n",
    "FUNDNO_WFICN = pd.read_stata(\"Data/FUNDO_WFICN.dta\")\n",
    "FUNDNO_WFICN2 = FUNDNO_WFICN[['fundno', 'wficn']]\n",
    "FUNDNO_WFICN2.columns = ['fundid(LSEG)', 'wficn']\n",
    "FUNDNO_WFICN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26097136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates and missing values\n",
    "FUNDNO_WFICN3 = FUNDNO_WFICN2.drop_duplicates(subset=['wficn', 'fundid(LSEG)'], keep='first').reset_index(drop=True)\n",
    "FUNDNO_WFICN3 = FUNDNO_WFICN3.dropna()\n",
    "FUNDNO_WFICN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3862c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with funds holdings data\n",
    "lseg_data4 = pd.merge(lseg_data3, FUNDNO_WFICN3, how = \"left\", on = 'fundid(LSEG)')\n",
    "lseg_data4 = lseg_data4.dropna()\n",
    "lseg_data4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de52fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make funds dataset\n",
    "merged_data = pd.merge(index_flag_funds2, lseg_data4, on=['Date', 'wficn'], how='inner')\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872af890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum over to get each stock passively held\n",
    "passive_data = (merged_data.groupby(['Date', 'ticker'])['shares_outstanding'].sum().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55370383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset data and rename columns for ease\n",
    "passive_data.columns = ['Date', 'ticker', 'shares_held_passive']\n",
    "passive_data_21 = passive_data.copy()\n",
    "passive_data_21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23875f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to csv so that don't need to load the big datasets again\n",
    "passive_data_21.to_csv(\"Data/passive_21.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180afcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data, new starting point\n",
    "passive_data_21 = pd.read_csv(\"Data/passive_21.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1437febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop na\n",
    "passive_data2 = passive_data_21.dropna()\n",
    "passive_data3 = passive_data2[['Date', 'ticker', 'shares_held_passive']]\n",
    "passive_data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e5f2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load earnings dates data\n",
    "earnings_dates = pd.read_excel(\"Data/LSEG_earnings.xlsx\")\n",
    "earnings_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c4c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename and subset columns for ease\n",
    "earnings_dates2 = earnings_dates[['oftic', 'Period End Date, SAS Format' ,'Announce Date, SAS Format', 'Announce time, SAS Format']]\n",
    "earnings_dates2.columns = ['ticker', 'quarter' ,'earnings_date', 'earnings_time']\n",
    "earnings_dates2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577ca1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to datetime format\n",
    "earnings_dates2['earnings_date'] = pd.to_datetime(earnings_dates2['earnings_date'])\n",
    "earnings_dates2['earnings_time'] = pd.to_datetime(earnings_dates2['earnings_time'], format='%H:%M:%S').dt.time\n",
    "earnings_dates2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b70fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if earnings time is after mkt close consider it next day\n",
    "earnings_dates2['nextday'] = earnings_dates2['earnings_time'] > pd.to_datetime('16:30:00', format='%H:%M:%S').time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c831c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change earnings date to next day if after mkt close\n",
    "if earnings_dates2['nextday'].any() == True:\n",
    "    earnings_dates2['earnings_date'] = earnings_dates2['earnings_date'] + pd.Timedelta(days=1)\n",
    "\n",
    "earnings_dates2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409e42aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset and add additional Date column for merging\n",
    "earnings_dates3 = earnings_dates2.drop(columns=['earnings_time', 'nextday', 'quarter'])\n",
    "earnings_dates3['Date'] = earnings_dates3['earnings_date']\n",
    "earnings_dates3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4127d59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load stock data\n",
    "stock_data = pd.read_stata(\"Data/daily_stock.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5ff6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset to relevant time and convert date to datetime format\n",
    "stock_data = stock_data[stock_data['date'] >= \"2021-01-01\"]\n",
    "stock_data['date'] = pd.to_datetime(stock_data['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293f9857",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns and create shares outstanding column\n",
    "stock_colnames = [\"permno\", \"Date\", \"ticker\", \"company\", \"cusip\", \"price\", \"vol\", \"return\", \"shares_outstnading(1000s)\"]\n",
    "stock_data.columns = stock_colnames\n",
    "stock_data[\"shares_outstanding\"] = stock_data[\"shares_outstnading(1000s)\"] * 1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3a2151",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset and drop na\n",
    "stock_data2 = stock_data.drop(\"shares_outstnading(1000s)\", axis = 1)\n",
    "stock_data3 = stock_data2.dropna()\n",
    "stock_data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe29ca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find negative prices\n",
    "negative_prices = stock_data3[(stock_data3.duplicated(subset=['ticker','Date'], keep=False)) &(stock_data3['price'] < 0)]\n",
    "negative_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ee57f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove negative prices\n",
    "stock_data3 = stock_data3[stock_data3['price'] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7dd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates keeping the one with highest shares outstanding, after check those seem more reasonable\n",
    "stock_data3 = stock_data3.sort_values(['ticker','Date','shares_outstanding'], ascending=[True, True, False])\n",
    "stock_data3 = stock_data3.drop_duplicates(subset=['ticker','Date'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fc30b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean data for merge\n",
    "passive_data3['ticker'] = passive_data3['ticker'].astype(str)\n",
    "passive_data3['ticker'] = passive_data3['ticker'].str.upper()\n",
    "stock_data3['ticker'] = stock_data3['ticker'].astype(str)\n",
    "stock_data3['ticker'] = stock_data3['ticker'].str.upper()\n",
    "passive_data3['Date'] = pd.to_datetime(passive_data3['Date'])\n",
    "stock_data3['Date'] = pd.to_datetime(stock_data3['Date'])\n",
    "passive_data3 = passive_data3.sort_values(['ticker', 'Date']).reset_index(drop=True)\n",
    "stock_data3 = stock_data3.sort_values(['ticker', 'Date']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb304e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge datasets\n",
    "merge1 = pd.merge(stock_data3, passive_data3, on = ['ticker', 'Date'], how = 'left')\n",
    "merge1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151deb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#backfill passive holdings for each stock for each quarter\n",
    "merge2 = merge1.copy()\n",
    "merge2['QuarterEnd'] = merge2['Date'].dt.to_period('Q').dt.end_time\n",
    "merge2['shares_held_passive'] = (merge2.groupby(['ticker', 'QuarterEnd'])['shares_held_passive']\n",
    "    .transform(lambda x: x.bfill() if x.notna().any() else x))\n",
    "merge2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9d4cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop na and create share passive column\n",
    "merge3 = merge2.dropna()\n",
    "merge3 = merge3.drop(columns=['QuarterEnd'])\n",
    "merge3['share_passive'] = merge3['shares_held_passive']/merge3['shares_outstanding']\n",
    "merge3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b644126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep data for merge\n",
    "earnings_dates3['ticker'] = earnings_dates3['ticker'].astype(str)\n",
    "earnings_dates3['ticker'] = earnings_dates3['ticker'].str.upper()\n",
    "earnings_dates3['Date'] = pd.to_datetime(earnings_dates3['Date'])\n",
    "earnings_dates3 = earnings_dates3.sort_values(['ticker', 'Date']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3476100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge\n",
    "merge4 = pd.merge(merge3, earnings_dates3, on = ['Date', 'ticker'], how = 'left')\n",
    "merge4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923da0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify stocks with no earnings dates\n",
    "no_earn_tickers = merge4.groupby('ticker')['earnings_date'].apply(lambda x: x.notna().any() == False)\n",
    "no_earn_tickers = no_earn_tickers[no_earn_tickers].index.tolist()\n",
    "len(no_earn_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c7ec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stocks with no earnings dates\n",
    "merge4 = merge4[~merge4['ticker'].isin(no_earn_tickers)].copy()\n",
    "merge4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984fcfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new column for next earnings date\n",
    "merge4['earnings_next'] = (merge4.groupby('ticker')['earnings_date'].transform(lambda x: x.bfill()))\n",
    "merge4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31568a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new column for previous earnings date\n",
    "merge4['earnings_prev'] = (merge4.groupby('ticker')['earnings_date'].transform(lambda x: x.ffill()))\n",
    "merge4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a592cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#days since previous and until next earnings date\n",
    "merge4['days_since_prev'] = (merge4['Date'] - merge4['earnings_prev']).dt.days\n",
    "merge4['days_until_next'] = (merge4['earnings_next'] - merge4['Date']).dt.days\n",
    "merge4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1815ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to choose nearest earnings date\n",
    "def choose_nearest(row):\n",
    "    # If both exist, pick the closer date in absolute time\n",
    "    if pd.notna(row['earnings_prev']) and pd.notna(row['earnings_next']):\n",
    "        return row['earnings_prev'] if abs(row['days_since_prev']) <= abs(row['days_until_next']) else row['earnings_next']\n",
    "    # If only one exists, use it\n",
    "    elif pd.notna(row['earnings_prev']):\n",
    "        return row['earnings_prev']\n",
    "    elif pd.notna(row['earnings_next']):\n",
    "        return row['earnings_next']\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e22eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the function\n",
    "merge4['earnings_date_nearest'] = merge4.apply(choose_nearest, axis=1)\n",
    "merge4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9b08e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create day to/from nearest earnings date\n",
    "merge4['days_from_earn'] = (merge4['Date'] - merge4['earnings_date_nearest']).dt.days\n",
    "merge4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5a11a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset data\n",
    "merge5 = merge4[['Date', 'ticker', 'price', 'vol', 'return', 'share_passive', 'days_from_earn']]\n",
    "merge5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93de3b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge5['return_lag1'] = merge5.groupby('ticker')['return'].shift(1)\n",
    "merge5['abnormal_return'] = abs(merge5['return'] - merge5['return_lag1'])\n",
    "merge5['sd_3day'] = merge5.groupby('ticker')['price'].rolling(window=3).std().reset_index(level=0, drop=True)\n",
    "merge5['3day_vol'] = merge5['sd_3day']/np.sqrt(3)\n",
    "merge5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61befb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create event window\n",
    "master_data = merge5[(merge5['days_from_earn'] >= -31) & (merge5['days_from_earn'] <= 15)]\n",
    "master_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb6d3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if any share passive greater than 1\n",
    "greaterthan1 = master_data[master_data['share_passive'] >= 1]\n",
    "problem_tickers = greaterthan1['ticker'].unique()\n",
    "totaltickers = master_data['ticker'].unique()\n",
    "print(f\"total:{len(totaltickers)}, problems: {len(problem_tickers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb685f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove those with share passive greater than 1\n",
    "master_data = master_data[master_data['share_passive'] < 0.95] #maybe 1.0 or 0.9?\n",
    "master_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed81c004",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_data.to_csv(\"Data/master_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
